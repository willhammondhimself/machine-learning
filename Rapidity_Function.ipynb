{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "407256a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 70000 uniform samples + 30000 boundary-biased samples = 100000 total\n",
      "Boundary sample range: x ∈ [-0.950, 0.950]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((100000,), (100000,))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pylab as pl\n",
    "import math as m\n",
    "\n",
    "n_data = 10**5  # Total 100K samples\n",
    "x_min, x_max = -0.95, 0.95\n",
    "y_min, y_max = -0.95, 0.95\n",
    "\n",
    "np.random.seed(42)  # Consistency\n",
    "\n",
    "# STRATIFIED SAMPLING - Phase 2 Implementation\n",
    "# 70% uniform samples\n",
    "n_uniform = int(0.7 * n_data)\n",
    "x_uniform = np.random.uniform(x_min, x_max, size=n_uniform)\n",
    "y_uniform = np.random.uniform(y_min, y_max, size=n_uniform)\n",
    "\n",
    "# 30% boundary-biased samples using Beta(0.5, 0.5)\n",
    "# Beta(0.5, 0.5) creates U-shaped distribution favoring extremes\n",
    "n_boundary = n_data - n_uniform\n",
    "beta_samples_x = np.random.beta(0.5, 0.5, size=n_boundary)\n",
    "beta_samples_y = np.random.beta(0.5, 0.5, size=n_boundary)\n",
    "\n",
    "# Scale Beta samples from [0,1] to [x_min, x_max]\n",
    "x_boundary = (x_max - x_min) * beta_samples_x + x_min\n",
    "y_boundary = (y_max - y_min) * beta_samples_y + y_min\n",
    "\n",
    "# Combine uniform and boundary-biased samples\n",
    "x_list = np.concatenate([x_uniform, x_boundary])\n",
    "y_list = np.concatenate([y_uniform, y_boundary])\n",
    "\n",
    "# Shuffle to mix uniform and boundary samples\n",
    "shuffle_indices = np.random.permutation(n_data)\n",
    "x_list = x_list[shuffle_indices]\n",
    "y_list = y_list[shuffle_indices]\n",
    "\n",
    "print(f\"Generated {n_uniform} uniform samples + {n_boundary} boundary-biased samples = {n_data} total\")\n",
    "print(f\"Boundary sample range: x ∈ [{x_boundary.min():.3f}, {x_boundary.max():.3f}]\")\n",
    "\n",
    "x_list.shape, y_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d0a4c23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100000, 1]), torch.Size([100000, 1]), torch.Size([100000, 1]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now make them Torch tensor\n",
    "import torch\n",
    "X_Input = torch.tensor(x_list, dtype=torch.float32)\n",
    "Y_Input = torch.tensor(y_list, dtype=torch.float32)\n",
    "XY_Input = X_Input*Y_Input\n",
    "\n",
    "X_Input, Y_Input, XY_Input = X_Input.unsqueeze(1), Y_Input.unsqueeze(1), XY_Input.unsqueeze(1),\n",
    "X_Input.shape, Y_Input.shape, XY_Input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c141a9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batches: 157\n",
      "Validation batches: 40\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100000, (tensor([0.0454]), tensor([0.9315]), tensor([0.0423])))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "# Combine input and target tensors into a dataset\n",
    "Total_Data = TensorDataset(X_Input, Y_Input, XY_Input)\n",
    "\n",
    "# Specify the size of the validation set\n",
    "validation_ratio = 0.20\n",
    "validation_size = int(validation_ratio * len(Total_Data))\n",
    "training_size = len(Total_Data) - validation_size\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "Training_Set, Validation_Set = random_split(Total_Data, [training_size, validation_size])\n",
    "\n",
    "BATCH_SIZE = 512 \n",
    "\n",
    "# Create data loaders\n",
    "Train_Loader = DataLoader(\n",
    "    Training_Set, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    num_workers=0  # Keep at 0 for MPS device\n",
    ")\n",
    "\n",
    "Valid_Loader = DataLoader(\n",
    "    Validation_Set, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=0  # Keep at 0 for MPS device\n",
    ")\n",
    "\n",
    "print(f\"Training batches: {len(Train_Loader)}\")\n",
    "print(f\"Validation batches: {len(Valid_Loader)}\")\n",
    "len(Total_Data), Total_Data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ece3615a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network parameters: 16,898\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (fc1): Linear(in_features=1, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (fc3): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (activation): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set device to MPS (Mac GPU)\n",
    "device = torch.device('mps')\n",
    "\n",
    "# Create the Network - Optimized for M4 Pro\n",
    "\n",
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(1, 128)    # Input layer to first hidden layer\n",
    "        self.fc2 = nn.Linear(128, 128)  # First hidden layer to second hidden layer  \n",
    "        self.fc3 = nn.Linear(128, 1)    # Output layer\n",
    "        self.activation = nn.Tanh()\n",
    "        \n",
    "        # Learnable output scale - helps fix magnitude collapse\n",
    "        self.output_scale = nn.Parameter(torch.tensor([180.0]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))  \n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = x * self.output_scale  # Scale the output\n",
    "        return x\n",
    "    \n",
    "Transform_NN = Network().to(device)\n",
    "\n",
    "print(f\"Network parameters: {sum(p.numel() for p in Transform_NN.parameters()):,}\")\n",
    "Transform_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fec0419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thanks Aiqing Zhu\n",
    "def Get_Grad(y, x, create_graph=True, keepdim=False):\n",
    "    '''\n",
    "    y: [N, Ny] or [Ny]\n",
    "    x: [N, Nx] or [Nx]\n",
    "    Return dy/dx ([N, Ny, Nx] or [Ny, Nx]).\n",
    "    '''\n",
    "    N = y.size(0) if len(y.size()) == 2 else 1\n",
    "    Ny = y.size(-1)\n",
    "    Nx = x.size(-1)\n",
    "    z = torch.ones_like(y[..., 0])\n",
    "    dy = []\n",
    "    for i in range(Ny):\n",
    "        dy.append(torch.autograd.grad(y[..., i], x, grad_outputs=z, create_graph=create_graph)[0])\n",
    "    shape = np.array([N, Ny])[2-len(y.size()):]\n",
    "    shape = list(shape) if keepdim else list(shape[shape > 1])\n",
    "    return torch.cat(dy, dim=-1).view(shape + [Nx])\n",
    "\n",
    "def Custom_Loss(model,X,Y,XY):\n",
    "\n",
    "    # L1 is F(X)+F(Y)-F(XY)=0\n",
    "    FX, FY, FXY = model(X), model(Y), model((X+Y)/(1+XY))\n",
    "    L1 = torch.mean((FX+FY-FXY)**2)\n",
    "\n",
    "    # Get the device from the model's parameters\n",
    "    device = next(model.parameters()).device\n",
    "    Xc = torch.tensor([0], dtype=torch.float32, device=device).requires_grad_(True)\n",
    "    FXc = model(Xc)\n",
    "    dFdXc = Get_Grad(FXc, Xc, create_graph=True, keepdim=False)\n",
    "\n",
    "    # L2 is fixing the value at xc=0 to be f(xc)=0\n",
    "    L2 = torch.mean(FXc**2)\n",
    "\n",
    "    # L3 is fixing the gradient at xc=0 to be df/dx=1\n",
    "    L3 = torch.mean((dFdXc-1)**2)\n",
    "    \n",
    "    return L1, L2, L3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba9a109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch    1 | Train Loss: 5876.682586 | Valid Loss: 0.815652 | LR: 1.00e-03\n",
      "Epoch  101 | Train Loss: 0.198729 | Valid Loss: 0.061554 | LR: 9.94e-04\n",
      "Epoch  201 | Train Loss: 2.293318 | Valid Loss: 0.075255 | LR: 9.75e-04\n",
      "Epoch  301 | Train Loss: 0.050263 | Valid Loss: 0.046740 | LR: 9.45e-04\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "\n",
    "# Enable faster matrix operations\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# define loss function and optimization method\n",
    "parameters = list(Transform_NN.parameters())\n",
    "optimizer = optim.Adam(parameters, lr=1e-3)\n",
    "\n",
    "\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=2000, eta_min=1e-6)\n",
    "\n",
    "# Loss weights: Emphasize boundary conditions (L2, L3) 100x more than physics equation (L1)\n",
    "c1, c2, c3 = 1, 100, 100\n",
    "\n",
    "# Define the number of epochs - increased from 1000 to 2000 for better convergence\n",
    "N_EPOCHS = 2000\n",
    "Epoch_List = np.arange(N_EPOCHS)+1\n",
    "Train_Loss_List = np.zeros((N_EPOCHS))\n",
    "Valid_Loss_List = np.zeros((N_EPOCHS))\n",
    "\n",
    "Train_L1_List = np.zeros((N_EPOCHS))\n",
    "Train_L2_List = np.zeros((N_EPOCHS))\n",
    "Train_L3_List = np.zeros((N_EPOCHS))\n",
    "\n",
    "# How frequent should it report answer\n",
    "N_REP = 100\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    Train_Loss, Train_L1, Train_L2, Train_L3 = 0.0, 0.0, 0.0, 0.0\n",
    "    \n",
    "    # Training loop\n",
    "    Transform_NN.train()\n",
    "    for id_batch, (X, Y, XY) in enumerate(Train_Loader):\n",
    "        # Ensure data is on the correct device with correct dtype\n",
    "        X = X.to(device, dtype=torch.float32)\n",
    "        Y = Y.to(device, dtype=torch.float32)\n",
    "        XY = XY.to(device, dtype=torch.float32)\n",
    "        \n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Calculate loss components\n",
    "        L1, L2, L3 = Custom_Loss(Transform_NN, X, Y, XY)\n",
    "        Loss = c1*L1 + c2*L2 + c3*L3\n",
    "        \n",
    "        # Backward pass\n",
    "        Loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate loss\n",
    "        Train_Loss += Loss.item()\n",
    "        Train_L1 += L1.item()\n",
    "        Train_L2 += L2.item()\n",
    "        Train_L3 += L3.item()\n",
    "        \n",
    "    # Average training losses\n",
    "    Train_Loss_List[epoch] = Train_Loss / len(Train_Loader)\n",
    "    Train_L1_List[epoch] = Train_L1 / len(Train_Loader)\n",
    "    Train_L2_List[epoch] = Train_L2 / len(Train_Loader)\n",
    "    Train_L3_List[epoch] = Train_L3 / len(Train_Loader)\n",
    "\n",
    "    # Validation loop - NOTE: We need gradients for Custom_Loss (to compute f'(0))\n",
    "    # So we DON'T use torch.no_grad() here\n",
    "    Transform_NN.eval()\n",
    "    Valid_Loss = 0.0\n",
    "    for X, Y, XY in Valid_Loader:\n",
    "        # Ensure data is on correct device\n",
    "        X = X.to(device, dtype=torch.float32)\n",
    "        Y = Y.to(device, dtype=torch.float32)\n",
    "        XY = XY.to(device, dtype=torch.float32)\n",
    "        \n",
    "        # Calculate loss (gradients needed for L3 = f'(0) term)\n",
    "        L1, L2, L3 = Custom_Loss(Transform_NN, X, Y, XY)\n",
    "        Loss = c1*L1 + c2*L2 + c3*L3\n",
    "        Valid_Loss += Loss.item()\n",
    "        \n",
    "    Valid_Loss_List[epoch] = Valid_Loss / len(Valid_Loader)\n",
    "\n",
    "    # Step the learning rate scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    # Print progress (including current learning rate)\n",
    "    if epoch % N_REP == 0:\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        print(f'Epoch {epoch+1:4d} | Train Loss: {Train_Loss_List[epoch]:.6f} | Valid Loss: {Valid_Loss_List[epoch]:.6f} | LR: {current_lr:.2e}')\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbe9be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab as pl\n",
    "\n",
    "f1 = pl.figure(figsize=(24,8))\n",
    "sf1_1, sf1_2, sf1_3 = f1.add_subplot(1,3,1), f1.add_subplot(1,3,2), f1.add_subplot(1,3,3)\n",
    "\n",
    "sf1_1.plot(Epoch_List,Train_Loss_List, linewidth=3, label='Training Loss')\n",
    "sf1_1.plot(Epoch_List,Valid_Loss_List, linewidth=3, label='Validation Loss')\n",
    "\n",
    "sf1_1.set_title('Learning Progress')\n",
    "sf1_1.set_xlabel('Epochs')\n",
    "sf1_1.set_ylabel('Loss')\n",
    "sf1_1.set_yscale('log')\n",
    "sf1_1.legend()\n",
    "\n",
    "sf1_2.plot(Epoch_List,Train_L1_List, linewidth=3, label='Train L1')\n",
    "sf1_2.plot(Epoch_List,Train_L2_List, linewidth=3, label='Train L2')\n",
    "sf1_2.plot(Epoch_List,Train_L3_List, linewidth=3, label='Train L3')\n",
    "sf1_2.set_title('Learning Progress')\n",
    "sf1_2.set_xlabel('Epochs')\n",
    "sf1_2.set_ylabel('Loss')\n",
    "sf1_2.set_yscale('log')\n",
    "sf1_2.legend()\n",
    "\n",
    "sf1_3.plot(Epoch_List,c1*Train_L1_List, linewidth=3, label='Train c1L1')\n",
    "sf1_3.plot(Epoch_List,c2*Train_L2_List, linewidth=3, label='Train c2L2')\n",
    "sf1_3.plot(Epoch_List,c3*Train_L3_List, linewidth=3, label='Train c3L3')\n",
    "sf1_3.set_title('Learning Progress')\n",
    "sf1_3.set_xlabel('Epochs')\n",
    "sf1_3.set_ylabel('Loss')\n",
    "sf1_3.set_yscale('log')\n",
    "sf1_3.legend()\n",
    "\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3892ab01",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the Learnt\n",
    "n_val = 10**4\n",
    "#z_min, z_max = x_min*y_min, x_max*y_max\n",
    "z_min, z_max = -0.8, 0.8\n",
    "\n",
    "Z_Input = torch.linspace(z_min, z_max, steps=n_val).unsqueeze(1).to(device)\n",
    "FNNZ = Transform_NN(Z_Input)\n",
    "\n",
    "Z_Input.shape, FNNZ.shape\n",
    "\n",
    "Z_numpy, FNNZ_numpy = Z_Input.cpu().detach().numpy(), FNNZ.cpu().detach().numpy()\n",
    "LogZ_numpy = np.arctanh(Z_numpy)\n",
    "\n",
    "f2 = pl.figure(figsize=(12,6))\n",
    "sf2_1 = f2.add_subplot(1,1,1)\n",
    "\n",
    "sf2_1.plot(Z_numpy, FNNZ_numpy, linestyle='-', color=(0,1,0), linewidth=4, label='Neural Network model')\n",
    "sf2_1.plot(Z_numpy, LogZ_numpy, linestyle=':',color=(0,0,1), linewidth=4, label=\"Rapidity\")\n",
    "sf2_1.set_xlabel('Values')\n",
    "sf2_1.set_ylabel('The Transformation')\n",
    "sf2_1.legend()\n",
    "\n",
    "pl.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
